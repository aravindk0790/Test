---
image: pythonimahe

default:
  tags:
    - runner
variables:
  AWS_ACCOUNT: 9cxxxx
  AWS_ROLE: vvvvv
  AWS_REGION: eu-west-2
  NAMESPACE: movvvvvv
  ENVIRONMENT: test
  HELM_CHART_VERSION: 0.41.4
  HTTP_PROXY: http://proxy.local.dwpcloud.uk:3128
  HTTPS_PROXY: http://proxy.local.dwpcloud.uk:3128
  http_proxy: http://proxy.local.dwpcloud.uk:3128
  https_proxy: http://proxy.local.dwpcloud.uk:3128

stages:
  - credentials
  # - quality
  - deploy

aws-credentials:
  stage: credentials
  before_script:
    # Install JQ
    - apt-get update -y
    - apt-get install jq -y
    # Install AWS CLI
    - curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
    - unzip awscliv2.zip
    - ./aws/install -i /usr/local/aws-cli -b /usr/local/bin
  script:
    # Assume ECR Role
    - aws sts get-caller-identity
    - ECR_ROLE=$(aws sts assume-role --role-arn arn:aws:iam::${AWS_ACCOUNT}:role/${AWS_ROLE} --role-session-name zimperium-hcs-runner)
    - echo $ECR_ROLE | jq -r .Credentials.AccessKeyId > AccessKeyId.txt
    - echo $ECR_ROLE | jq -r .Credentials.SecretAccessKey > SecretAccessKey.txt
    - echo $ECR_ROLE | jq -r .Credentials.SessionToken > SessionToken.txt
    - aws sts get-caller-identity
  artifacts:
    paths:
    - ./AccessKeyId.txt
    - ./SecretAccessKey.txt
    - ./SessionToken.txt

deploy-test:
  stage: deploy
  image:
    name: zegl/kube-score:latest-helm3@sha256ffffffffff
    entrypoint: [""]
  script:
    - apk add --no-cache aws-cli jq
    - apk update  && apk add --no-cache curl
    - curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.23.17/2023-03-17/bin/linux/amd64/kubectl
    - chmod +x ./kubectl
    - mkdir -p $HOME/bin && cp ./kubectl $HOME/bin/kubectl && export PATH=$HOME/bin:$PATH
    - mv ./kubectl /usr/local/bin/kubectl
    # - mkdir -p $HOME/.kube
    - echo 'export PATH=$HOME/bin:$PATH' >> ~/.bashrc
    - kubectl version --short --client
    # Assume EKS Namespace Role
    - TEMP_ROLE=$(aws sts assume-role --role-arn arn:aws:iam::${AWS_ACCOUNT}:role/${AWS_ROLE} --role-session-name hcs-eks-runner-eks)
    - export AWS_ACCESS_KEY_ID=$(echo $TEMP_ROLE | jq -r .Credentials.AccessKeyId)
    - export AWS_SECRET_ACCESS_KEY=$(echo $TEMP_ROLE | jq -r .Credentials.SecretAccessKey)
    - export AWS_SESSION_TOKEN=$(echo $TEMP_ROLE | jq -r .Credentials.SessionToken)
    - aws sts get-caller-identity
    # Login to ECR
    - export HELM_EXPERIMENTAL_OCI=1
    - aws ecr get-login-password --region eu-west-2 | helm registry login --username AWS --password-stdin 11111111.dkr.ecr.eu-west-2.amazonaws.com
    # Login to EKS Cluster
    - aws eks update-kubeconfig --region eu-west-2 --name pdu-test-eks-cluster
    - kubectl config view
    - kubectl config current-context
    - kubectl config set-context --current --namespace=$NAMESPACE

    # Create common EFS volume used by Content, ZConsole & Wotification-Worker
    - kubectl apply -f zimp_efs_pvc.yaml -n mobility-zimperium-apps
    # Configure network policy to allow pods to communicate and ingress from ISTIO gateway
    - kubectl apply -f ${CI_PROJECT_DIR}/zimp_network_policy.yaml --namespace=$NAMESPACE
    # Workaround - deploy dummy email smtp server
    - kubectl apply -f ${CI_PROJECT_DIR}/zimp_smtp_stub.yaml --namespace=$NAMESPACE

    # It will show status of the ongoing operation
    - helm version 
    - helm ls -a -n mobility-zimperium-apps
    # #- helm rollback <release> 0.1.0 -n $NAMESPACE --wait 
    - chmod +x ./bin/install.sh 

    # # # Run Zimperium installation script
    - bin/install.sh -n $NAMESPACE
    # Configure Ingress to Zimperium via HCS ISTIO gateway
    - kubectl apply -f ${CI_PROJECT_DIR}/zimp_ingress_test.yaml --namespace=$NAMESPACE

    # install redis component separately second time
    - helm upgrade -i elasticcache -n mobility-zimperium-apps k8s/helm/elasticcache --values global.yaml --values k8s_manifest.yaml --wait
