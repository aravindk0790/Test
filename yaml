---
image:

default:
  tags:
    - namr
variables:
  AWS_ACCOUNT: EKS Dev/Test account
  AWS_ROLE: 
  AWS_REGION: 
  NAMESPACE: 
  ENVIRONMENT: test
  HELM_CHART_VERSION: 0.41.4

stages:
  - credentials
  # - quality
  - deploy

aws-credentials:
  stage: credentials
  before_script:
    # Install JQ
    - apt-get update -y
    - apt-get install jq -y
    # Install AWS CLI
    - curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
    - unzip awscliv2.zip
    - ./aws/install -i /usr/local/aws-cli -b /usr/local/bin

deploy-test:
  stage: deploy
  image:
    name: xxxxxx
    entrypoint: [""]
  script:
    - apk add --no-cache aws-cli jq
    - apk update  && apk add --no-cache curl
    - curl xxxxxxxxx
    - chmod +x ./kubectl
    - mkdir -p $HOME/bin && cp ./kubectl $HOME/bin/kubectl && export PATH=$HOME/bin:$PATH
    - mv ./kubectl /usr/local/bin/kubectl
    - echo 'export PATH=$HOME/bin:$PATH' >> ~/.bashrc
    - kubectl version --short --client
    # Assume EKS Namespace Role
    - TEMP_ROLE=$(aws sts assume-role --role-arn arn:aws:iam::${AWS_ACCOUNT}:role/${AWS_ROLE} --role-session-name hcs-eks-runner-eks)
    - export AWS_ACCESS_KEY_ID=$(echo $TEMP_ROLE | jq -r .Credentials.AccessKeyId)
    - export AWS_SECRET_ACCESS_KEY=$(echo $TEMP_ROLE | jq -r .Credentials.SecretAccessKey)
    - export AWS_SESSION_TOKEN=$(echo $TEMP_ROLE | jq -r .Credentials.SessionToken)
    - aws sts get-caller-identity
    # Login to ECR
    - export HELM_EXPERIMENTAL_OCI=1
    - aws ecr get-login-password --region eu-west-2 | helm registry login --username AWS --password-stdin xxxxxxxx.dkr.ecr.eu-west-2.amazonaws.com
    # Login to EKS Cluster
    - aws eks update-kubeconfig --region eu-west-2 --name pdu-test-eks-cluster
    - kubectl config view
    - kubectl config current-context
    - kubectl config set-context --current --namespace=$NAMESPACE

    # Create common EFS volume used by Content, ZConsole & Wotification-Worker
    - kubectl apply -f zimp_efs_pvc.yaml -n mobility-zimperium-apps
    # Configure network policy to allow pods to communicate and ingress from ISTIO gateway
    - kubectl apply -f ${CI_PROJECT_DIR}/zimp_network_policy.yaml --namespace=$NAMESPACE
    # install redis component separately
    - helm upgrade -i elasticcache -n mobility-zimperium-apps k8s/helm/elasticcache --values global.yaml --values k8s_manifest.yaml --wait --debug 
    # Workaround - deploy dummy email smtp server
    - kubectl apply -f ${CI_PROJECT_DIR}/zimp_smtp_stub.yaml --namespace=$NAMESPACE


    # It will show status of the ongoing operation
    - helm version 
    - helm ls -a -n mobility-zimperium-apps
    # #- helm rollback <release> 0.1.0 -n $NAMESPACE --wait 
    - chmod +x ./bin/install.sh 

    # # # Run Zimperium installation script
    - bin/install.sh -n $NAMESPACE
    # Configure Ingress to Zimperium via HCS ISTIO gateway
    - kubectl apply -f ${CI_PROJECT_DIR}/zimp_ingress_test.yaml --namespace=$NAMESPACE

